#!/usr/bin/env python3
"""Generate a Markdown report from experiment summary JSON.

Usage:
  python generate_report.py <run_id>
"""

import json
import sys
from pathlib import Path


def main():
    if len(sys.argv) < 2:
        print("Usage: python generate_report.py <run_id>", file=sys.stderr)
        sys.exit(1)

    run_id = sys.argv[1]
    results_dir = Path(__file__).parent / "results"
    summary_file = results_dir / f"{run_id}_summary.json"

    if not summary_file.exists():
        print(f"Error: summary not found at {summary_file}", file=sys.stderr)
        print("Run 'python compute_metrics.py <run_id>' first.", file=sys.stderr)
        sys.exit(1)

    summary = json.loads(summary_file.read_text())
    baselines = summary["baselines"]

    lines = []
    lines.append(f"# Verifier Loop Experiment Report")
    lines.append(f"")
    lines.append(f"**Run ID**: {summary['run_id']}  ")
    lines.append(f"**Timestamp**: {summary['timestamp']}  ")
    lines.append(f"**Language**: {summary['language']}  ")
    lines.append(f"**Max Retries**: {summary['max_retries']}  ")
    lines.append(f"**Total Prompts**: {summary['total_prompts']}")
    lines.append(f"")
    lines.append(f"---")
    lines.append(f"")

    # Headline metrics table
    lines.append(f"## Headline Metrics")
    lines.append(f"")
    header = "| Metric |"
    sep = "|--------|"
    for b in baselines:
        mode = b["feedback_mode"]
        header += f" {mode} |"
        sep += "--------|"
    lines.append(header)
    lines.append(sep)

    metrics = [
        ("pass@1", lambda b: f"{b['pass_at_1']}%"),
        ("pass@k", lambda b: f"{b['pass_at_k']}%" if b["feedback_mode"] != "none" else "--"),
        ("Mean retries", lambda b: str(b["mean_retries_to_pass"]) if b["feedback_mode"] != "none" else "--"),
        ("Median retries", lambda b: str(b["median_retries_to_pass"]) if b["feedback_mode"] != "none" else "--"),
        ("Mean latency", lambda b: f"{b['mean_latency_seconds']}s"),
        ("p95 latency", lambda b: f"{b['p95_latency_seconds']}s"),
    ]

    for label, fmt in metrics:
        row = f"| {label} |"
        for b in baselines:
            row += f" {fmt(b)} |"
        lines.append(row)

    lines.append(f"")
    lines.append(f"---")
    lines.append(f"")

    # Per-template breakdown
    lines.append(f"## Per-Template Breakdown")
    lines.append(f"")

    # Collect all template IDs
    all_templates = set()
    for b in baselines:
        all_templates.update(b["template_breakdown"].keys())
    all_templates = sorted(all_templates)

    header = "| Template |"
    sep = "|----------|"
    for b in baselines:
        mode = b["feedback_mode"]
        header += f" {mode} pass@1 | {mode} pass@k |"
        sep += "--------|--------|"
    lines.append(header)
    lines.append(sep)

    for tid in all_templates:
        row = f"| {tid} |"
        for b in baselines:
            td = b["template_breakdown"].get(tid, {})
            p1 = td.get("pass_at_1", "--")
            pk = td.get("pass_at_k", "--")
            p1_str = f"{p1}%" if isinstance(p1, (int, float)) else str(p1)
            pk_str = f"{pk}%" if isinstance(pk, (int, float)) else str(pk)
            if b["feedback_mode"] == "none":
                pk_str = "--"
            row += f" {p1_str} | {pk_str} |"
        lines.append(row)

    lines.append(f"")
    lines.append(f"---")
    lines.append(f"")

    # Failure analysis
    lines.append(f"## Failure Analysis")
    lines.append(f"")

    for b in baselines:
        hist = b["failure_histogram"]
        if not hist:
            continue
        lines.append(f"### {b['feedback_mode']} baseline")
        lines.append(f"")
        total_failures = sum(hist.values())
        lines.append(f"Total failures: {total_failures}")
        lines.append(f"")
        lines.append(f"| Category | Count | % of Failures |")
        lines.append(f"|----------|-------|---------------|")
        for cat, count in sorted(hist.items(), key=lambda x: -x[1]):
            pct = round(count / total_failures * 100, 1) if total_failures else 0
            lines.append(f"| {cat} | {count} | {pct}% |")
        lines.append(f"")

    lines.append(f"---")
    lines.append(f"")
    lines.append(f"*Generated by Grammar Oracle experiment harness*")

    report = "\n".join(lines)
    report_file = results_dir / f"{run_id}_report.md"
    report_file.write_text(report)
    print(f"Report written to {report_file}")


if __name__ == "__main__":
    main()
